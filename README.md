# BigData-Project
---

# ğŸš— US Accidents Analysis with PySpark

This project uses **PySpark** to perform large-scale data processing and extract insightful trends from the **US Accidents Dataset** sourced from [Kaggle](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents/data). It demonstrates the power of distributed data processing to handle and analyze millions of records efficiently.

---


#### ğŸ“Š Dataset (Updated)

- **Name**: US Accidents (7.7 million records)
- **Source**: [Kaggle - US Accidents](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents/data)
- **Size**: ~**3.06 GB** CSV file
- **Scope**: Covers ~7.7 million accidents across the US from 2016 to 2023
- **Features**: 
  - 47 columns including `Severity`, `Start_Time`, `City`, `State`, `Temperature`, `Visibility`, `Weather_Condition`, etc.
  - Many fields have **thousands of unique values** (e.g., over 350 unique weather conditions, thousands of city/street names)
  
#### âš ï¸ Note

Due to its size and number of unique entries, this dataset is a **great fit for PySpark**, as it:
- Exceeds the comfortable range for in-memory Pandas processing
- Benefits from parallelized data transformations and filtering
- Can reveal insights at scale (e.g., trends over time, geographic clustering, severity prediction)

---

## ğŸ”§ Project Structure

```bash
us-accidents-pyspark/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ US_Accidents.csv          # Downloaded dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb         # Initial EDA using PySpark
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py          # Data cleaning & transformation
â”‚   â”œâ”€â”€ insights.py               # Spark jobs for analysis
â”‚   â””â”€â”€ utils.py                  # Helper functions
â”‚
â”œâ”€â”€ output/
â”‚   â””â”€â”€ visualizations/           # Plots and charts
â”‚
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ README.md                     # Project overview
â””â”€â”€ run_analysis.py               # Main script to run analysis
```

---

## ğŸš€ Getting Started

### 1. Clone the repo

```bash
git clone https://github.com/Person-Ad/BigData-Project
cd BigData-Project
```

### 2. Install dependencies

Create a virtual environment and install dependencies:

```bash
pip install -r requirements.txt
```

Make sure you have **PySpark** installed:

```bash
pip install pyspark
```

### 3. Download the dataset

Download `US_Accidents.csv` from [Kaggle](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents/data) and place it inside the `data/` directory.

### 4. Run the analysis

```bash
python run_analysis.py
```

---

## ğŸ“ˆ Key Analyses

- Accident distribution by **state**, **city**, **time**, and **weather**
- **Top 10 accident-prone cities**
- Accidents by **severity level**
- Impact of **weather conditions** (rain, snow, fog, etc.)
- Temporal trends (hour of day, day of week, monthly)
- Heatmaps and visualizations

---

## ğŸ›  Technologies Used

- **PySpark** â€“ distributed data processing
- **Pandas/Matplotlib/Seaborn** â€“ for comparative visualization
- **Jupyter Notebook** â€“ for exploratory data analysis
- **Python 3.8+**

---

## ğŸ“Œ Sample Insights

- California and Florida report the most accidents.
- Majority of accidents occur during rush hours (7-9 AM, 4-6 PM).
- Rain and foggy weather increase the likelihood of severe accidents.
- Most accidents are of **Severity 2** (on a 1â€“4 scale).

---

## ğŸ“š References

- [Kaggle Dataset](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents/data)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)

---

## ğŸ¤ Contributions

Contributions are welcome! Open an issue or submit a PR to enhance this project.

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

