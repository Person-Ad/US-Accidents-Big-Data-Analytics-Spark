{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "ba77808d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "sys.path.append(str(Path(\"../..\").resolve()))\n",
                "\n",
                "from src.data_ingestion import *\n",
                "from src.data_preprocessing import *\n",
                "\n",
                "from pyspark.sql import DataFrame\n",
                "from pyspark.sql.functions import col\n",
                "from pyspark.sql.types import NumericType, StringType\n",
                "from pyspark.sql import functions as F\n",
                "\n",
                "import seaborn as sns\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "from itertools import combinations\n",
                "\n",
                "from scipy import stats\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import pandas as pd\n",
                "from pyspark.sql.window import Window\n",
                "\n",
                "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
                "from pyspark.ml import Pipeline\n",
                "from pyspark.ml.classification import RandomForestClassifier\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "256eacb6",
            "metadata": {},
            "outputs": [],
            "source": [
                "spark = init_spark()\n",
                "df = load_data(spark, \"../../data/US_Accidents_March23.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "f03766c1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Features with one unique value: []\n"
                    ]
                }
            ],
            "source": [
                "df = preprocess_data(df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "69902e48",
            "metadata": {},
            "outputs": [],
            "source": [
                "df = df.withColumn(\"Hour\", F.hour(\"Start_Time\"))\n",
                "df = df.withColumn(\"DayOfWeek\", F.dayofweek(\"Start_Time\"))  # Sunday = 1\n",
                "df = df.withColumn(\"Month\", F.month(\"Start_Time\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "0844c794",
            "metadata": {},
            "outputs": [],
            "source": [
                "df = df.drop(\"Start_Lat\", \"Start_Lng\", \"Start_Time\", \"End_Time\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "3e5f98c1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Define categorical columns for encoding\n",
                "categorical_cols = [\"State\", \"Timezone\", \"Weather_Condition\", \"Sunrise_Sunset\"]\n",
                "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
                "encoders = [OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_Vec\") for col in categorical_cols]\n",
                "\n",
                "# 5. Define numeric + boolean + derived time columns\n",
                "numeric_cols = [\n",
                "    \"Distance(mi)\", \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\",\n",
                "    \"Visibility(mi)\", \"Wind_Speed(mph)\", \"Hour\", \"DayOfWeek\", \"Month\"\n",
                "]\n",
                "\n",
                "boolean_cols = [\n",
                "    \"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\", \"Junction\", \"No_Exit\", \"Railway\",\n",
                "    \"Roundabout\", \"Station\", \"Stop\", \"Traffic_Calming\", \"Traffic_Signal\"\n",
                "]\n",
                "\n",
                "# Fill null boolean columns with False (assumption)\n",
                "for col in boolean_cols:\n",
                "    df = df.withColumn(col, F.when(F.col(col).isNull(), F.lit(False)).otherwise(F.col(col).cast(\"boolean\")))\n",
                "\n",
                "# 6. Assemble all features into a vector\n",
                "assembler_inputs = numeric_cols + boolean_cols + [col + \"_Vec\" for col in categorical_cols]\n",
                "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
                "\n",
                "# 7. Define the full pipeline\n",
                "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
                "\n",
                "# 8. Fit and transform the data\n",
                "model_ready_df = pipeline.fit(df).transform(df)\n",
                "\n",
                "# 9. Select final dataset for modeling\n",
                "final_df = model_ready_df.select(\"features\", \"Severity\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02e64087",
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
                "\n",
                "rf = RandomForestClassifier(labelCol=\"Severity\", featuresCol=\"features\")\n",
                "rf_model = rf.fit(train_df)\n",
                "predictions = rf_model.transform(test_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "518cf62b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy:  0.7965\n",
                        "F1 Score:  0.7063\n",
                        "Precision: 0.6345\n",
                        "Recall:    0.7965\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
                "\n",
                "# Accuracy\n",
                "accuracy_eval = MulticlassClassificationEvaluator(\n",
                "    labelCol=\"Severity\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
                ")\n",
                "accuracy = accuracy_eval.evaluate(predictions)\n",
                "\n",
                "# F1 Score\n",
                "f1_eval = MulticlassClassificationEvaluator(\n",
                "    labelCol=\"Severity\", predictionCol=\"prediction\", metricName=\"f1\"\n",
                ")\n",
                "f1 = f1_eval.evaluate(predictions)\n",
                "\n",
                "# Precision\n",
                "precision_eval = MulticlassClassificationEvaluator(\n",
                "    labelCol=\"Severity\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
                ")\n",
                "precision = precision_eval.evaluate(predictions)\n",
                "\n",
                "# Recall\n",
                "recall_eval = MulticlassClassificationEvaluator(\n",
                "    labelCol=\"Severity\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
                ")\n",
                "recall = recall_eval.evaluate(predictions)\n",
                "\n",
                "print(f\"Accuracy:  {accuracy:.4f}\")\n",
                "print(f\"F1 Score:  {f1:.4f}\")\n",
                "print(f\"Precision: {precision:.4f}\")\n",
                "print(f\"Recall:    {recall:.4f}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "9206a179",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "ERROR:root:Exception while sending command.\n",
                        "Traceback (most recent call last):\n",
                        "  File \"c:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
                        "    answer = smart_decode(self.stream.readline()[:-1])\n",
                        "                          ~~~~~~~~~~~~~~~~~~~~^^\n",
                        "  File \"c:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py\", line 719, in readinto\n",
                        "    return self._sock.recv_into(b)\n",
                        "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
                        "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
                        "\n",
                        "During handling of the above exception, another exception occurred:\n",
                        "\n",
                        "Traceback (most recent call last):\n",
                        "  File \"c:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
                        "    response = connection.send_command(command)\n",
                        "  File \"c:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
                        "    raise Py4JNetworkError(\n",
                        "        \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
                        "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
                    ]
                },
                {
                    "ename": "ConnectionRefusedError",
                    "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
                        "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      7\u001b[39m ovr = OneVsRest(classifier=lr, labelCol=\u001b[33m\"\u001b[39m\u001b[33mSeverity\u001b[39m\u001b[33m\"\u001b[39m, featuresCol=\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m ovr_model = \u001b[43movr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\classification.py:3586\u001b[39m, in \u001b[36mOneVsRest._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m   3584\u001b[39m pool = ThreadPool(processes=\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.getParallelism(), numClasses))\n\u001b[32m-> \u001b[39m\u001b[32m3586\u001b[39m models = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minheritable_thread_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainSingleClass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumClasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handlePersistence:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\pool.py:367\u001b[39m, in \u001b[36mPool.map\u001b[39m\u001b[34m(self, func, iterable, chunksize)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03mApply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03min a list that is returned.\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\pool.py:48\u001b[39m, in \u001b[36mmapstar\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmapstar\u001b[39m(args):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\util.py:342\u001b[39m, in \u001b[36minheritable_thread_target.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m SparkContext._active_spark_context._jsc.sc().setLocalProperties(properties)\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\classification.py:3582\u001b[39m, in \u001b[36mOneVsRest._fit.<locals>.trainSingleClass\u001b[39m\u001b[34m(index)\u001b[39m\n\u001b[32m   3581\u001b[39m     paramMap[cast(HasWeightCol, classifier).weightCol] = weightCol\n\u001b[32m-> \u001b[39m\u001b[32m3582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMap\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'No connection could be made because the target machine actively refused it', None, 10061, None))",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
                        "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:2184\u001b[39m, in \u001b[36mInteractiveShell.showtraceback\u001b[39m\u001b[34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[39m\n\u001b[32m   2181\u001b[39m         traceback.print_exc()\n\u001b[32m   2182\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_pdb:\n\u001b[32m   2186\u001b[39m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28mself\u001b[39m.debugger(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\zmqshell.py:559\u001b[39m, in \u001b[36mZMQInteractiveShell._showtraceback\u001b[39m\u001b[34m(self, etype, evalue, stb)\u001b[39m\n\u001b[32m    553\u001b[39m sys.stdout.flush()\n\u001b[32m    554\u001b[39m sys.stderr.flush()\n\u001b[32m    556\u001b[39m exc_content = {\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype.\u001b[34m__name__\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    562\u001b[39m dh = \u001b[38;5;28mself\u001b[39m.displayhook\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\protocol.py:471\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    470\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    473\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
                        "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it"
                    ]
                }
            ],
            "source": [
                "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
                "\n",
                "# Define base classifier\n",
                "lr = LogisticRegression(labelCol=\"Severity\", featuresCol=\"features\", maxIter=10)\n",
                "\n",
                "# Wrap in One-vs-Rest\n",
                "ovr = OneVsRest(classifier=lr, labelCol=\"Severity\", featuresCol=\"features\")\n",
                "ovr_model = ovr.fit(train_df)\n",
                "\n",
                "# Predictions\n",
                "ovr_predictions = ovr_model.transform(test_df)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "458e18d6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
                "\n",
                "# Loop over each class to compute AUC individually\n",
                "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
                "\n",
                "# Store AUCs\n",
                "auc_scores = []\n",
                "\n",
                "for i in range(4):  # assuming classes are [1, 2, 3, 4]\n",
                "    bin_df = test_df.withColumn(\"label\", F.when(F.col(\"Severity\") == i, 1.0).otherwise(0.0))\n",
                "    bin_model = LogisticRegression(labelCol=\"label\", featuresCol=\"features\").fit(train_df.withColumn(\"label\", F.when(F.col(\"Severity\") == i, 1.0).otherwise(0.0)))\n",
                "    bin_pred = bin_model.transform(bin_df)\n",
                "    auc = evaluator.evaluate(bin_pred)\n",
                "    auc_scores.append((f\"Class {i}\", auc))\n",
                "\n",
                "# Show AUC for each class\n",
                "for cls, auc in auc_scores:\n",
                "    print(f\"{cls} AUC: {auc:.4f}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e0d7c372",
            "metadata": {},
            "source": [
                "# Binary Classification for Severity "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "4cdf191d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import when\n",
                "\n",
                "binary_df = final_df.withColumn(\n",
                "    \"label\", when(F.col(\"Severity\").isin(3, 4), 1).otherwise(0)\n",
                ")\n",
                "small_df = binary_df.sample(fraction=0.2, seed=42)\n",
                "train_df, test_df = small_df.randomSplit([0.8, 0.2], seed=42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "7929a0f5",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.ml.classification import (\n",
                "    LogisticRegression, RandomForestClassifier,\n",
                "    GBTClassifier, DecisionTreeClassifier\n",
                ")\n",
                "\n",
                "models = {\n",
                "    \"Logistic Regression\": LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10),\n",
                "    \"Random Forest\": RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20),\n",
                "    \"Gradient-Boosted Trees\": GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=20),\n",
                "    \"Decision Tree\": DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=5),\n",
                "}\n",
                "\n",
                "# Train all models\n",
                "fitted_models = {name: model.fit(train_df) for name, model in models.items()}\n",
                "# Predict with each\n",
                "predictions = {name: model.transform(test_df) for name, model in fitted_models.items()}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "babc55d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Logistic Regression: AUC=0.7297, F1=0.7492, Accuracy=0.8077\n",
                        "Random Forest: AUC=0.7509, F1=0.7184, Accuracy=0.8053\n",
                        "Gradient-Boosted Trees: AUC=0.8089, F1=0.7660, Accuracy=0.8192\n",
                        "Decision Tree: AUC=0.6452, F1=0.7438, Accuracy=0.8112\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
                "\n",
                "def evaluate_model(name, pred_df):\n",
                "    evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
                "    evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
                "    evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
                "\n",
                "    auc = evaluator_auc.evaluate(pred_df)\n",
                "    f1 = evaluator_f1.evaluate(pred_df)\n",
                "    acc = evaluator_acc.evaluate(pred_df)\n",
                "\n",
                "    return {\"Model\": name, \"AUC\": auc, \"F1 Score\": f1, \"Accuracy\": acc}\n",
                "\n",
                "results = [evaluate_model(name, pred_df) for name, pred_df in predictions.items()]\n",
                "for r in results:\n",
                "    print(f\"{r['Model']}: AUC={r['AUC']:.4f}, F1={r['F1 Score']:.4f}, Accuracy={r['Accuracy']:.4f}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "7a5fb7e8",
            "metadata": {},
            "outputs": [
                {
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling o1981.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelWriter.saveImpl(LogisticRegression.scala:1311)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m model_save_paths = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLogistic Regression\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m./logistic_regression_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRandom Forest\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m./random_forest_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGradient-Boosted Trees\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m./gbt_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDecision Tree\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m./decision_tree_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m }\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m fitted_models.items():\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\util.py:262\u001b[39m, in \u001b[36mMLWritable.save\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    261\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\util.py:213\u001b[39m, in \u001b[36mJavaMLWriter.save\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
                        "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1981.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelWriter.saveImpl(LogisticRegression.scala:1311)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
                    ]
                }
            ],
            "source": [
                "model_save_paths = {\n",
                "    \"Logistic Regression\": \"./logistic_regression_model\",\n",
                "    \"Random Forest\": \"./random_forest_model\",\n",
                "    \"Gradient-Boosted Trees\": \"./gbt_model\",\n",
                "    \"Decision Tree\": \"./decision_tree_model\",\n",
                "}\n",
                "\n",
                "for name, model in fitted_models.items():\n",
                "    model.save(model_save_paths[name])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "25cdf498",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}