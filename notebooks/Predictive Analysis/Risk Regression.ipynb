{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba77808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "\n",
    "from src.data_ingestion import *\n",
    "from src.data_preprocessing import *\n",
    "from src.descriptive_analytics import *\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import NumericType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "256eacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "df = load_data(spark, \"../../data/US_Accidents_March23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f03766c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef3dc1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Metrics:\n",
      "RMSE: 317208.9238\n",
      "MAE: 200413.4663\n",
      "R2: 0.5838\n",
      "\n",
      "Feature Importance:\n",
      "Avg_Visibility: 0.0034\n",
      "Avg_Precipitation: 0.9957\n",
      "Avg_Temperature: 0.0007\n",
      "Avg_Accident_Distance: 0.0001\n",
      "Num_Unique_Cities: 0.0000\n",
      "+-----+------------------+------------------+\n",
      "|State|Risk_Score        |prediction        |\n",
      "+-----+------------------+------------------+\n",
      "|MN   |415289.0          |498768.71314730117|\n",
      "|NJ   |314331.0          |288799.1398893113 |\n",
      "|DC   |39936.0           |52651.29227274125 |\n",
      "|NE   |62955.99999999999 |8818.791077522372 |\n",
      "|NC   |721657.0          |803182.9571533577 |\n",
      "|MO   |185545.0          |243860.13372003625|\n",
      "|IL   |402678.00000000006|839219.4194777964 |\n",
      "|MS   |35589.0           |80191.04793966173 |\n",
      "|OH   |278007.0          |784559.627471513  |\n",
      "|NY   |786233.0          |830852.8840830902 |\n",
      "+-----+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "state_features = df.groupBy(\"State\").agg(\n",
    "    F.avg(\"Visibility(mi)\").alias(\"Avg_Visibility\"),\n",
    "    F.avg(F.when(F.col(\"Sunrise_Sunset\") == \"Night\", 1).otherwise(0)).alias(\"Prop_Night_Accidents\"),\n",
    "    F.avg(\"Precipitation(in)\").alias(\"Avg_Precipitation\"),\n",
    "    F.avg(\"Temperature(F)\").alias(\"Avg_Temperature\"),\n",
    "    F.avg(\"Distance(mi)\").alias(\"Avg_Accident_Distance\"),\n",
    "    F.countDistinct(\"City\").alias(\"Num_Unique_Cities\"),\n",
    "    F.avg(F.unix_timestamp(\"End_Time\") - F.unix_timestamp(\"Start_Time\")).alias(\"Avg_Accident_Duration_Seconds\"),\n",
    "    F.count(\"*\").alias(\"Total_Accidents\"),\n",
    "    F.avg(\"Severity\").alias(\"Avg_Severity\")\n",
    ")\n",
    "\n",
    "state_features = state_features.withColumn(\n",
    "    \"Risk_Score\",\n",
    "    F.col(\"Total_Accidents\") * F.col(\"Avg_Severity\")\n",
    ")\n",
    "\n",
    "state_features = state_features.drop(\"Total_Accidents\", \"Avg_Severity\")\n",
    "\n",
    "state_features = state_features.cache()\n",
    "\n",
    "feature_cols = [\n",
    "    \"Avg_Visibility\", \"Avg_Precipitation\", \"Avg_Temperature\",\n",
    "    \"Avg_Accident_Distance\", \"Num_Unique_Cities\"\n",
    "]\n",
    "state_features = state_features.na.drop(subset=feature_cols + [\"Risk_Score\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "model = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Risk_Score\",\n",
    "    maxIter=100,\n",
    "    regParam=0.0\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, model])\n",
    "\n",
    "train_df, test_df = state_features.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "try:\n",
    "    model_fitted = pipeline.fit(train_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    spark.stop()\n",
    "    raise e\n",
    "\n",
    "predictions = model_fitted.transform(test_df)\n",
    "\n",
    "evaluators = {\n",
    "    \"rmse\": RegressionEvaluator(labelCol=\"Risk_Score\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "    \"mae\": RegressionEvaluator(labelCol=\"Risk_Score\", predictionCol=\"prediction\", metricName=\"mae\"),\n",
    "    \"r2\": RegressionEvaluator(labelCol=\"Risk_Score\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "}\n",
    "\n",
    "print(\"Regression Metrics:\")\n",
    "for metric, evaluator in evaluators.items():\n",
    "    value = evaluator.evaluate(predictions)\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")\n",
    "\n",
    "lr_model = model_fitted.stages[-1]\n",
    "coefficients = lr_model.coefficients.toArray()\n",
    "abs_coefficients = [abs(coef) for coef in coefficients]\n",
    "total = sum(abs_coefficients)\n",
    "importance = [coef / total if total > 0 else 0 for coef in abs_coefficients]\n",
    "print(\"\\nFeature Importance:\")\n",
    "for feature, imp in zip(feature_cols, importance):\n",
    "    print(f\"{feature}: {imp:.4f}\")\n",
    "\n",
    "predictions.select(\"State\", \"Risk_Score\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
