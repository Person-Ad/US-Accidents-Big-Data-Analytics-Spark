{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "\n",
    "from src.data_ingestion import *\n",
    "from src.data_preprocessing import *\n",
    "\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import NumericType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "256eacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "df = load_data(spark, \"../../data/US_Accidents_March23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f03766c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3dc1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Metrics:\n",
      "AUC-ROC: 0.7778\n",
      "Accuracy: 0.8182\n",
      "Weightedprecision: 0.8182\n",
      "Weightedrecall: 0.8182\n",
      "F1: 0.8182\n",
      "\n",
      "Feature Importance:\n",
      "Avg_Precipitation: -505.8624\n",
      "Avg_Accident_Distance: -1.0414\n",
      "Avg_Visibility: -0.7750\n",
      "Avg_Temperature: 0.2671\n",
      "Num_Unique_Cities: 0.0154\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "state_features = df.groupBy(\"State\").agg(\n",
    "    F.avg(\"Visibility(mi)\").alias(\"Avg_Visibility\"),\n",
    "    F.avg(F.when(F.col(\"Sunrise_Sunset\") == \"Night\", 1).otherwise(0)).alias(\"Prop_Night_Accidents\"),\n",
    "    F.avg(\"Precipitation(in)\").alias(\"Avg_Precipitation\"),\n",
    "    F.avg(\"Temperature(F)\").alias(\"Avg_Temperature\"),\n",
    "    F.avg(\"Distance(mi)\").alias(\"Avg_Accident_Distance\"),\n",
    "    F.countDistinct(\"City\").alias(\"Num_Unique_Cities\"),\n",
    "    F.avg(F.unix_timestamp(\"End_Time\") - F.unix_timestamp(\"Start_Time\")).alias(\"Avg_Accident_Duration_Seconds\"),\n",
    "    F.count(\"*\").alias(\"Total_Accidents\"),\n",
    "    F.avg(\"Severity\").alias(\"Avg_Severity\")\n",
    ")\n",
    "\n",
    "state_features = state_features.withColumn(\n",
    "    \"Risk_Score\",\n",
    "    F.col(\"Total_Accidents\") * F.col(\"Avg_Severity\")\n",
    ")\n",
    "\n",
    "state_features = state_features.drop(\n",
    "    \"Total_Accidents\",\n",
    "    \"Avg_Severity\"\n",
    ")\n",
    "\n",
    "quantile_75 = state_features.approxQuantile(\"Risk_Score\", [0.75], 0.0)[0]\n",
    "\n",
    "# Create binary label: 1 if Risk_Score > 75th percentile, 0 otherwise\n",
    "state_features = state_features.withColumn(\n",
    "    \"Is_High_Risk\",\n",
    "    F.when(F.col(\"Risk_Score\") > quantile_75, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "\n",
    "feature_cols = [\n",
    "    \"Avg_Visibility\", \"Avg_Precipitation\", \"Avg_Temperature\",\n",
    "    \"Avg_Accident_Distance\", \"Num_Unique_Cities\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"  # Skip rows with nulls in feature columns\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Is_High_Risk\",\n",
    "    maxIter=100,\n",
    "    regParam=0.0,\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, model])\n",
    "train_df, test_df = state_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Is_High_Risk\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "metrics = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]\n",
    "evaluators = {\n",
    "    metric: MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Is_High_Risk\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=metric\n",
    "    ) for metric in metrics\n",
    "}\n",
    "\n",
    "print(\"Classification Metrics:\")\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "for metric, evaluator in evaluators.items():\n",
    "    value = evaluator.evaluate(predictions)\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Step 10: Extract Feature Importance using Coefficients\n",
    "coefficients = model.stages[-1].coefficients\n",
    "importance_pairs = [(feature_cols[i], coefficients[i]) for i in range(len(feature_cols))]\n",
    "importance_pairs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "for feature, importance in importance_pairs:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
