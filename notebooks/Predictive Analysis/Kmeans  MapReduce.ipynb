{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import findspark\n",
    "findspark.init() # Find Spark installation\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType, BooleanType, TimestampType, StructType, StructField\n",
    "\n",
    "# For ML tasks (even if demonstrating MapReduce concepts, preprocessing often uses MLlib)\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, Bucketizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix # For potential matrix operations\n",
    "\n",
    "import math\n",
    "import heapq\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "\n",
    "from src.data_ingestion import *\n",
    "from src.data_preprocessing import *\n",
    "from src.descriptive_analytics import *\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256eacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "df = load_data(spark, \"../../data/US_Accidents_March23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a601e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek, month, year, when, col, sqrt\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import sqrt as math_sqrt\n",
    "\n",
    "# Step 1: Preprocessing and Feature Engineering\n",
    "df = df.withColumn(\"hour_of_day\", hour(col(\"Start_Time\")))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"Start_Time\")))\n",
    "df = df.withColumn(\"month\", month(col(\"Start_Time\")))\n",
    "df = df.withColumn(\"year\", year(col(\"Start_Time\")))\n",
    "df = df.withColumn(\"weather_condition_cat\", \n",
    "                   when(col(\"Weather_Condition\") == \"Clear\", 0)\n",
    "                   .when(col(\"Weather_Condition\") == \"Rain\", 1)\n",
    "                   .when(col(\"Weather_Condition\") == \"Snow\", 2)\n",
    "                   .otherwise(3))\n",
    "df = df.withColumn(\"is_night\", \n",
    "                   when((col(\"hour_of_day\") >= 18) | (col(\"hour_of_day\") < 6), 1).otherwise(0))\n",
    "df = df.withColumn(\"severe_accident\", \n",
    "                   when(col(\"Severity\") >= 3, 1).otherwise(0))\n",
    "\n",
    "# Impute missing values\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"Temperature(F)\", \"Wind_Speed(mph)\", \"Humidity(%)\"],\n",
    "    outputCols=[\"Temperature_imputed\", \"Wind_Speed_imputed\", \"Humidity_imputed\"]\n",
    ")\n",
    "df = imputer.fit(df).transform(df)\n",
    "\n",
    "# Drop rows with missing critical columns\n",
    "df = df.dropna(subset=[\"Severity\", \"Start_Lat\", \"Start_Lng\"])\n",
    "\n",
    "# Step 2: Prepare RDD for MapReduce\n",
    "# Convert DataFrame to RDD of (point, metadata) tuples\n",
    "# point: [Start_Lat, Start_Lng], metadata: other columns for analysis\n",
    "rdd = df.select(\"Start_Lat\", \"Start_Lng\", \"Severity\", \"weather_condition_cat\", \n",
    "                \"is_night\", \"severe_accident\", \"City\", \"State\")\\\n",
    "        .rdd.map(lambda row: (\n",
    "            [float(row[\"Start_Lat\"]), float(row[\"Start_Lng\"])],\n",
    "            {\n",
    "                \"Severity\": row[\"Severity\"],\n",
    "                \"weather_condition_cat\": row[\"weather_condition_cat\"],\n",
    "                \"is_night\": row[\"is_night\"],\n",
    "                \"severe_accident\": row[\"severe_accident\"],\n",
    "                \"City\": row[\"City\"],\n",
    "                \"State\": row[\"State\"]\n",
    "            }\n",
    "        ))\n",
    "\n",
    "# Step 3: Initialize Cluster Centers\n",
    "k = 5\n",
    "num_iterations = 10\n",
    "# Randomly select k points as initial centers\n",
    "initial_centers = rdd.takeSample(False, k, seed=42)\n",
    "centers = [point for point, _ in initial_centers]\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return math_sqrt(sum((a - b) ** 2 for a, b in zip(point1, point2)))\n",
    "\n",
    "# Step 4: MapReduce K-means Implementation\n",
    "for iteration in range(num_iterations):\n",
    "    # Map Phase: Assign each point to the nearest center\n",
    "    def map_to_nearest_center(point_metadata):\n",
    "        point, metadata = point_metadata\n",
    "        min_distance = float(\"inf\")\n",
    "        closest_center_idx = 0\n",
    "        for idx, center in enumerate(centers):\n",
    "            distance = euclidean_distance(point, center)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_center_idx = idx\n",
    "        return (closest_center_idx, (point, metadata))\n",
    "    \n",
    "    clustered_rdd = rdd.map(map_to_nearest_center)\n",
    "    \n",
    "    # Reduce Phase: Compute new centers by averaging points in each cluster\n",
    "    def reduce_centers(data):\n",
    "        cluster_idx, points_metadata = data\n",
    "        points = [p for p, _ in points_metadata]\n",
    "        count = len(points)\n",
    "        if count == 0:\n",
    "            return (cluster_idx, centers[cluster_idx])  # Keep old center if no points\n",
    "        # Sum coordinates\n",
    "        sum_coords = [0.0] * len(points[0])\n",
    "        for point in points:\n",
    "            for i, coord in enumerate(point):\n",
    "                sum_coords[i] += coord\n",
    "        # Average coordinates\n",
    "        new_center = [coord / count for coord in sum_coords]\n",
    "        return (cluster_idx, new_center)\n",
    "    \n",
    "    new_centers_rdd = clustered_rdd.groupByKey().map(reduce_centers)\n",
    "    new_centers = new_centers_rdd.collectAsMap()\n",
    "    \n",
    "    # Update centers\n",
    "    centers = [new_centers.get(i, centers[i]) for i in range(k)]\n",
    "    \n",
    "    print(f\"Iteration {iteration + 1}: Centers updated\")\n",
    "\n",
    "# Step 5: Assign Final Clusters\n",
    "def assign_final_cluster(point_metadata):\n",
    "    point, metadata = point_metadata\n",
    "    min_distance = float(\"inf\")\n",
    "    closest_center_idx = 0\n",
    "    for idx, center in enumerate(centers):\n",
    "        distance = euclidean_distance(point, center)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_center_idx = idx\n",
    "    return {\n",
    "        \"Start_Lat\": point[0],\n",
    "        \"Start_Lng\": point[1],\n",
    "        \"cluster\": closest_center_idx,\n",
    "        \"Severity\": metadata[\"Severity\"],\n",
    "        \"weather_condition_cat\": metadata[\"weather_condition_cat\"],\n",
    "        \"is_night\": metadata[\"is_night\"],\n",
    "        \"severe_accident\": metadata[\"severe_accident\"],\n",
    "        \"City\": metadata[\"City\"],\n",
    "        \"State\": metadata[\"State\"]\n",
    "    }\n",
    "\n",
    "final_rdd = rdd.map(assign_final_cluster)\n",
    "df_clustered = spark.createDataFrame(final_rdd)\n",
    "\n",
    "# Step 6: Visualize Clusters\n",
    "df_pd = df_clustered.select(\"Start_Lat\", \"Start_Lng\", \"cluster\", \"Severity\", \n",
    "                           \"weather_condition_cat\", \"is_night\", \"severe_accident\", \n",
    "                           \"City\", \"State\").toPandas()\n",
    "\n",
    "cluster_centers = np.array(centers)\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(df_pd['Start_Lat'], df_pd['Start_Lng'], c=df_pd['cluster'], \n",
    "                     cmap='viridis', alpha=0.5, s=10)\n",
    "plt.colorbar(scatter, label='Cluster Label')\n",
    "plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='x', color='red', \n",
    "           s=100, label=\"Cluster Centers\")\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.title('Geographical Clusters of US Accidents (MapReduce K-means)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Analyze Clusters for Insights\n",
    "# Compute cluster size, average severity, and night/severe ratios\n",
    "cluster_summary = df_clustered.groupBy(\"cluster\")\\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"Accident_Count\"),\n",
    "        F.avg(\"Severity\").alias(\"Avg_Severity\"),\n",
    "        F.avg(\"is_night\").alias(\"Night_Accident_Ratio\"),\n",
    "        F.avg(\"severe_accident\").alias(\"Severe_Accident_Ratio\")\n",
    "    )\n",
    "\n",
    "# Compute top weather condition per cluster\n",
    "weather_summary = df_clustered.groupBy(\"cluster\", \"weather_condition_cat\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"cluster\", col(\"count\").desc())\\\n",
    "    .groupBy(\"cluster\")\\\n",
    "    .agg(\n",
    "        F.first(\"weather_condition_cat\").alias(\"Top_Weather_Code\"),\n",
    "        F.first(\"count\").alias(\"Weather_Count\")\n",
    "    )\\\n",
    "    .withColumn(\"Top_Weather\", \n",
    "                when(col(\"Top_Weather_Code\") == 0, \"Clear\")\n",
    "                .when(col(\"Top_Weather_Code\") == 1, \"Rain\")\n",
    "                .when(col(\"Top_Weather_Code\") == 2, \"Snow\")\n",
    "                .otherwise(\"Other\"))\n",
    "\n",
    "# Find closest city/state to each cluster center\n",
    "centers_df = spark.createDataFrame(\n",
    "    [(i, float(center[0]), float(center[1])) for i, center in enumerate(centers)],\n",
    "    [\"center_cluster\", \"center_lat\", \"center_lng\"]\n",
    ")\n",
    "df_with_centers = df_clustered.crossJoin(centers_df)\\\n",
    "    .withColumn(\"distance\", \n",
    "                sqrt((col(\"Start_Lat\") - col(\"center_lat\"))**2 + \n",
    "                     (col(\"Start_Lng\") - col(\"center_lng\"))**2).cast(DoubleType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build a DataFrame of your centers, tagged with the same cluster ID\n",
    "centers_df = spark.createDataFrame(\n",
    "    [(i, float(center[0]), float(center[1])) for i, center in enumerate(centers)],\n",
    "    [\"center_cluster\", \"center_lat\", \"center_lng\"]\n",
    ")\n",
    "\n",
    "# 2) Join each accident point to its cluster center\n",
    "#    â€” Use an inner join on df_clustered.cluster == centers_df.center_cluster\n",
    "df_with_centers = (\n",
    "    df_clustered.alias(\"d\")\n",
    "      .join(centers_df.alias(\"c\"),\n",
    "            F.col(\"d.cluster\") == F.col(\"c.center_cluster\"))\n",
    "      .withColumn(\n",
    "          \"distance\",\n",
    "          F.sqrt(\n",
    "            (F.col(\"d.Start_Lat\") - F.col(\"c.center_lat\"))**2 +\n",
    "            (F.col(\"d.Start_Lng\") - F.col(\"c.center_lng\"))**2\n",
    "          ).cast(DoubleType())\n",
    "      )\n",
    ")\n",
    "\n",
    "# 3) For each cluster, find the minimum distance\n",
    "min_distances = (\n",
    "    df_with_centers\n",
    "      .groupBy(\"cluster\")\n",
    "      .agg(F.min(\"distance\").alias(\"min_distance\"))\n",
    ")\n",
    "\n",
    "# 4) Grab the City/State for whatever point realizes that min distance\n",
    "closest_points = (\n",
    "    df_with_centers\n",
    "      .join(min_distances, on=\"cluster\")\n",
    "      .where(F.col(\"distance\") == F.col(\"min_distance\"))\n",
    "      .select(\"cluster\", \"City\", \"State\")\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "# Show to verify\n",
    "closest_points.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine insights\n",
    "cluster_insights = cluster_summary.join(weather_summary, \"cluster\")\\\n",
    "                                 .join(closest_points, \"cluster\")\\\n",
    "                                 .select(\n",
    "                                     \"cluster\", \"Accident_Count\", \"Avg_Severity\", \n",
    "                                     \"Night_Accident_Ratio\", \"Severe_Accident_Ratio\", \n",
    "                                     \"Top_Weather\", \"Weather_Count\", \"City\", \"State\"\n",
    "                                 )\n",
    "\n",
    "# Display insights\n",
    "cluster_insights.show(truncate=False)\n",
    "\n",
    "# Save insights\n",
    "cluster_insights.toPandas().to_csv(\"cluster_insights_mapreduce.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6bb6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to Pandas for easy plotting\n",
    "cluster_insights_pd = cluster_insights.toPandas()\n",
    "\n",
    "# Plot Accident Count by Cluster\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(cluster_insights_pd['cluster'], cluster_insights_pd['Accident_Count'], color='skyblue')\n",
    "plt.title('Accident Count by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Accident Count')\n",
    "plt.xticks(cluster_insights_pd['cluster'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Average Severity by Cluster\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(cluster_insights_pd['cluster'], cluster_insights_pd['Avg_Severity'], color='lightcoral')\n",
    "plt.title('Average Severity by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Average Severity')\n",
    "plt.xticks(cluster_insights_pd['cluster'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
